import openai
import json
import os
import shutil
import re
import io
import contextlib
import traceback
from pathlib import Path
from urllib.parse import quote
import subprocess
import sys
import tempfile
import requests
import threading
import http.server
from functools import partial
import socketserver
from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Query
from fastapi.responses import JSONResponse, Response
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Optional
import httpx
import uvicorn
import os
import re
import json
from fastapi.responses import StreamingResponse
import os
import re
from copy import deepcopy
import openai
from fastapi import FastAPI, Body
from fastapi.responses import StreamingResponse

import re

os.environ.setdefault("MPLBACKEND", "Agg")


def ensure_common_imports(code_str: str) -> str:
    """Inject missing common data-analysis imports into Python code.

    - Adds `import pandas as pd` if `pd.` used or no pandas import and 'pandas' operations expected.
    - Adds `import matplotlib.pyplot as plt` if `plt.` used or plotting functions detected without import.
    - Adds `import seaborn as sns` if `sns.` used or seaborn function names appear.
    - Ensures non-interactive backend via `matplotlib.use('Agg')` when plt exists.
    """
    try:
        lines = code_str.splitlines()
        src = "\n".join(lines)

        has_pd = bool(re.search(r"(^|\n)\s*import\s+pandas\s+as\s+pd\b|(^|\n)\s*from\s+pandas\b", src))
        has_plt = bool(re.search(r"(^|\n)\s*import\s+matplotlib\.pyplot\s+as\s+plt\b|(^|\n)\s*from\s+matplotlib\s+import\s+pyplot\b", src))
        has_sns = bool(re.search(r"(^|\n)\s*import\s+seaborn\s+as\s+sns\b|(^|\n)\s*from\s+seaborn\b", src))

        needs_pd = (not has_pd) and bool(re.search(r"\bpd\.|read_csv\(|read_excel\(", src))
        needs_plt = (not has_plt) and bool(re.search(r"\bplt\.|plot\(|bar\(|hist\(|scatter\(", src))
        needs_sns = (not has_sns) and bool(re.search(r"\bsns\.|seaborn|scatterplot\(|lineplot\(|barplot\(", src))

        inject = []
        # Prefer ordering: pandas, matplotlib, seaborn
        if needs_pd:
            inject.append("import pandas as pd")
        if needs_plt:
            inject.append("import matplotlib")
            inject.append("matplotlib.use('Agg')")
            inject.append("import matplotlib.pyplot as plt")
        if needs_sns:
            inject.append("import seaborn as sns")

        if not inject:
            return code_str

        # Place imports at the very top (before code), keep original content intact
        return "\n".join(inject) + "\n" + code_str
    except Exception:
        # Fallback: do not alter code if detection fails
        return code_str

def execute_code(code_str):
    import io
    import contextlib
    import traceback

    stdout_capture = io.StringIO()
    stderr_capture = io.StringIO()
    try:
        with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(
            stderr_capture
        ):
            # è‡ªåŠ¨è¡¥å…¨å¸¸ç”¨æ•°æ®åˆ†æåº“å¯¼å…¥
            code_str = ensure_common_imports(code_str)
            exec(code_str, {})
        output = stdout_capture.getvalue()
        if stderr_capture.getvalue():
            output += stderr_capture.getvalue()
        return output
    except Exception as exec_error:
        code_lines = code_str.splitlines()
        tb_lines = traceback.format_exc().splitlines()
        error_line = None
        for line in tb_lines:
            if 'File "<string>", line' in line:
                try:
                    line_num = int(line.split(", line ")[1].split(",")[0])
                    error_line = line_num
                    break
                except (IndexError, ValueError):
                    continue
        error_message = f"Traceback (most recent call last):\n"
        if error_line is not None and 1 <= error_line <= len(code_lines):
            error_message += f'  File "<string>", line {error_line}, in <module>\n'
            error_message += f"    {code_lines[error_line-1].strip()}\n"
        error_message += f"{type(exec_error).__name__}: {str(exec_error)}"
        if stderr_capture.getvalue():
            error_message += f"\n{stderr_capture.getvalue()}"
        return f"[Error]:\n{error_message.strip()}"


def execute_code_safe(
    code_str: str, workspace_dir: str = None, timeout_sec: int = 120
) -> str:
    """åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­æ‰§è¡Œä»£ç ï¼Œæ”¯æŒè¶…æ—¶ï¼Œé¿å…é˜»å¡ä¸»è¿›ç¨‹ã€‚"""
    if workspace_dir is None:
        workspace_dir = WORKSPACE_BASE_DIR
    exec_cwd = os.path.abspath(workspace_dir)
    os.makedirs(exec_cwd, exist_ok=True)
    tmp_path = None
    try:
        fd, tmp_path = tempfile.mkstemp(suffix=".py", dir=exec_cwd)
        os.close(fd)
        with open(tmp_path, "w", encoding="utf-8") as f:
            f.write(ensure_common_imports(code_str))
        print(
            f"[exec] Running script: {tmp_path} (timeout={timeout_sec}s) cwd={exec_cwd}"
        )
        # åœ¨å­è¿›ç¨‹ä¸­è®¾ç½®æ— ç•Œé¢ç¯å¢ƒå˜é‡ï¼Œé¿å… GUI åç«¯
        child_env = os.environ.copy()
        child_env.setdefault("MPLBACKEND", "Agg")
        child_env.setdefault("QT_QPA_PLATFORM", "offscreen")
        child_env.pop("DISPLAY", None)

        completed = subprocess.run(
            [sys.executable, tmp_path],
            cwd=exec_cwd,
            capture_output=True,
            text=True,
            timeout=timeout_sec,
            env=child_env,
        )
        output = (completed.stdout or "") + (completed.stderr or "")
        return output
    except subprocess.TimeoutExpired:
        return f"[Timeout]: execution exceeded {timeout_sec} seconds"
    except Exception as e:
        return f"[Error]: {str(e)}"
    finally:
        try:
            if tmp_path and os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def execute_code_stream(
    code_str: str, workspace_dir: str = None, timeout_sec: int = 120
):
    """ä»¥æµå¼æ–¹å¼æ‰§è¡Œä»£ç ï¼Œé€æ­¥è¿”å›è¾“å‡ºå†…å®¹ã€‚

    - åœ¨ç‹¬ç«‹å­è¿›ç¨‹ä¸­è¿è¡Œè„šæœ¬ï¼Œå®æ—¶è¯»å– stdout/stderrã€‚
    - è‡ªåŠ¨æ³¨å…¥æ— ç•Œé¢ç¯å¢ƒå˜é‡ï¼Œé¿å… GUI é˜»å¡ã€‚
    - è¶…æ—¶åç»ˆæ­¢è¿›ç¨‹å¹¶è¿”å›è¶…æ—¶æç¤ºã€‚
    """
    import time
    import queue

    if workspace_dir is None:
        workspace_dir = WORKSPACE_BASE_DIR
    exec_cwd = os.path.abspath(workspace_dir)
    os.makedirs(exec_cwd, exist_ok=True)
    tmp_path = None

    # çº¿ç¨‹å®‰å…¨é˜Ÿåˆ—ï¼Œç”¨äºæ”¶é›†è¾“å‡º
    q: queue.Queue[str] = queue.Queue()

    def _reader(stream, prefix: str = ""):
        try:
            for line in iter(stream.readline, ""):
                if not line:
                    break
                # é€è¡Œæ¨é€ï¼Œä¿ç•™åŸå§‹æ¢è¡Œ
                q.put((prefix + line))
        except Exception as _:
            pass

    start_ts = time.time()
    try:
        # å†™å…¥ä¸´æ—¶è„šæœ¬
        fd, tmp_path = tempfile.mkstemp(suffix=".py", dir=exec_cwd)
        os.close(fd)
        with open(tmp_path, "w", encoding="utf-8") as f:
            f.write(ensure_common_imports(code_str))

        # å­è¿›ç¨‹ç¯å¢ƒ
        child_env = os.environ.copy()
        child_env.setdefault("MPLBACKEND", "Agg")
        child_env.setdefault("QT_QPA_PLATFORM", "offscreen")
        child_env.pop("DISPLAY", None)

        # ä½¿ç”¨ -u å¼ºåˆ¶æ— ç¼“å†²è¾“å‡º
        proc = subprocess.Popen(
            [sys.executable, "-u", tmp_path],
            cwd=exec_cwd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            env=child_env,
            bufsize=1,
        )

        # å¯åŠ¨è¯»å–çº¿ç¨‹
        t_out = threading.Thread(target=_reader, args=(proc.stdout, ""), daemon=True)
        t_err = threading.Thread(target=_reader, args=(proc.stderr, ""), daemon=True)
        t_out.start()
        t_err.start()

        last_heartbeat = 0.0
        heartbeat_interval = 1.0

        # ä¸»å¾ªç¯ï¼šä»é˜Ÿåˆ—å–å‡ºå¹¶ yieldï¼›é—´éš”å¿ƒè·³æç¤º
        while True:
            # è¶…æ—¶æ§åˆ¶
            if time.time() - start_ts > timeout_sec:
                try:
                    proc.kill()
                except Exception:
                    pass
                yield "[Timeout]: execution exceeded %d seconds\n" % timeout_sec
                break

            try:
                line = q.get(timeout=0.2)
                yield line
            except queue.Empty:
                # è‹¥è¿›ç¨‹ä»åœ¨è¿è¡Œä¸”ä¸€æ®µæ—¶é—´æ²¡æœ‰è¾“å‡ºï¼Œç»™å¿ƒè·³æç¤º
                if proc.poll() is None:
                    now = time.time()
                    if now - last_heartbeat >= heartbeat_interval:
                        last_heartbeat = now
                        yield "[status] Executing...\n"
                else:
                    # è¿›ç¨‹å·²ç»“æŸï¼Œé˜Ÿåˆ—ä¹Ÿç©ºï¼Œé€€å‡ºå¾ªç¯
                    break

        # æ’ç©ºå‰©ä½™è¾“å‡º
        while True:
            try:
                line = q.get_nowait()
                yield line
            except queue.Empty:
                break

    except Exception as e:
        yield f"[Error]: {str(e)}\n"
    finally:
        try:
            if tmp_path and os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


# API endpoint and model path (auto-detect Ollama first, fallback to mock server)
def _detect_ollama_base() -> tuple[str, str]:
    try:
        # Prefer OpenAI-compatible endpoint when available
        resp = requests.get("http://localhost:11434/api/tags", timeout=1.0)
        if resp.ok:
            data = resp.json()
            models = {m.get("name") for m in data.get("models", [])}
            if "deepseek-r1:1.5b" in models:
                return "http://localhost:11434/v1", "deepseek-r1:1.5b"
    except Exception:
        pass
    # Fallback to local mock server
    return "http://localhost:8000/v1", "RUC-DataLab/DeepAnalyze-8B"

API_BASE, MODEL_PATH = _detect_ollama_base()


# Initialize OpenAI client
client = openai.OpenAI(base_url=API_BASE, api_key="dummy")

# Workspace directory
WORKSPACE_BASE_DIR = "workspace"
HTTP_SERVER_PORT = 8100
HTTP_SERVER_BASE = (
    f"http://localhost:{HTTP_SERVER_PORT}"  # you can replace localhost to your local ip
)


def get_session_workspace(session_id: str) -> str:
    """è¿”å›æŒ‡å®š session çš„ workspace è·¯å¾„ï¼ˆworkspace/{session_id}/ï¼‰ã€‚"""
    if not session_id:
        session_id = "default"
    session_dir = os.path.join(WORKSPACE_BASE_DIR, session_id)
    os.makedirs(session_dir, exist_ok=True)
    return session_dir


def build_download_url(rel_path: str) -> str:
    try:
        encoded = quote(rel_path, safe="/")
    except Exception:
        encoded = rel_path
    return f"{HTTP_SERVER_BASE}/{encoded}"


# FastAPI app
app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def start_http_server():
    """å¯åŠ¨HTTPæ–‡ä»¶æœåŠ¡å™¨ï¼ˆä¸ä¿®æ”¹å…¨å±€å·¥ä½œç›®å½•ï¼‰ã€‚"""
    os.makedirs(WORKSPACE_BASE_DIR, exist_ok=True)
    handler = partial(
        http.server.SimpleHTTPRequestHandler, directory=WORKSPACE_BASE_DIR
    )
    with socketserver.TCPServer(("", HTTP_SERVER_PORT), handler) as httpd:
        print(f"HTTP Server serving {WORKSPACE_BASE_DIR} at port {HTTP_SERVER_PORT}")
        httpd.serve_forever()


# Start HTTP server in a separate thread
threading.Thread(target=start_http_server, daemon=True).start()


def collect_file_info(directory: str) -> str:
    """æ”¶é›†æ–‡ä»¶ä¿¡æ¯"""
    all_file_info_str = ""
    dir_path = Path(directory)
    if not dir_path.exists():
        return ""

    files = sorted([f for f in dir_path.iterdir() if f.is_file()])
    for idx, file_path in enumerate(files, start=1):
        size_bytes = os.path.getsize(file_path)
        size_kb = size_bytes / 1024
        size_str = f"{size_kb:.1f}KB"
        file_info = {"name": file_path.name, "size": size_str}
        file_info_str = json.dumps(file_info, indent=4, ensure_ascii=False)
        all_file_info_str += f"File {idx}:\n{file_info_str}\n\n"
    return all_file_info_str


def collect_rich_data_context(directory: str, max_files: int = 5, max_rows: int = 5) -> str:
    """æ”¶é›†æ›´ä¸°å¯Œçš„æ•°æ®ä¸Šä¸‹æ–‡ï¼šæ–‡ä»¶åã€å¤§å°ã€ç±»å‹ã€ä»¥åŠCSV/XLSXçš„åˆ—åä¸æ ·ä¾‹è¡Œã€‚

    ä»…å¯¹å·²çŸ¥ç»“æ„ï¼ˆCSV/XLSXï¼‰æä¾›è½»é‡è§£æï¼Œé¿å…åŠ è½½å·¨å¤§æ–‡ä»¶ï¼›å…¶ä»–ç±»å‹ä»…ç»™åŸºæœ¬ä¿¡æ¯ã€‚
    """
    dir_path = Path(directory)
    if not dir_path.exists():
        return ""

    items = []
    files = sorted([f for f in dir_path.iterdir() if f.is_file()])[:max_files]
    for file_path in files:
        size_bytes = os.path.getsize(file_path)
        size_kb = size_bytes / 1024
        size_str = f"{size_kb:.1f}KB"
        entry = {
            "name": file_path.name,
            "size": size_str,
            "type": file_path.suffix.lower()
        }

        # CSV é¢„è§ˆï¼šåˆ—å + å‰ N è¡Œæ ·ä¾‹
        if file_path.suffix.lower() == ".csv":
            try:
                import csv
                with open(file_path, "r", newline="", encoding="utf-8") as f:
                    reader = csv.reader(f)
                    rows = []
                    header = next(reader, [])
                    for i, r in enumerate(reader):
                        if i >= max_rows:
                            break
                        rows.append(r)
                entry["columns"] = header
                entry["sample_rows"] = rows
            except Exception:
                entry["columns"] = []
                entry["sample_rows"] = []
        # XLSX ç®€é¢„è§ˆï¼šåˆ—åï¼ˆè‹¥ pandas å¯ç”¨ï¼‰
        elif file_path.suffix.lower() == ".xlsx":
            try:
                import pandas as pd  # å¯é€‰
                df = pd.read_excel(file_path, nrows=max_rows)
                entry["columns"] = list(df.columns)
                entry["sample_rows"] = df.head(max_rows).values.tolist()
            except Exception:
                entry["columns"] = []
                entry["sample_rows"] = []

        items.append(entry)

    # æ ¼å¼åŒ–ä¸ºç´§å‡‘ JSON å­—ç¬¦ä¸²ï¼Œä¾¿äºæ¨¡å‹ç†è§£
    try:
        ctx = json.dumps({"files": items}, ensure_ascii=False, indent=2)
    except Exception:
        ctx = str(items)
    return ctx


def collect_rich_data_context_for(
    workspace_dir: str,
    selected_rel_paths: Optional[List[str]] = None,
    max_files: int = 5,
    max_rows: int = 5,
) -> str:
    """åŸºäº workspace ç›®å½•ä¸å¯é€‰çš„ç›¸å¯¹è·¯å¾„åˆ—è¡¨ï¼Œæ”¶é›† JSON æ ¼å¼çš„æ•°æ®ä¸Šä¸‹æ–‡ã€‚

    - è‹¥æä¾› selected_rel_pathsï¼šä»…åŒ…å«è¿™äº›æ–‡ä»¶ï¼ˆå­˜åœ¨ä¸”ä¸ºæ–‡ä»¶ï¼‰ã€‚
    - å¦åˆ™ï¼šæšä¸¾ workspace æ ¹ç›®å½•ä¸‹çš„å‰ max_files ä¸ªæ–‡ä»¶ã€‚
    - å¯¹ CSV/XLSX æä¾›åˆ—åä¸æ ·ä¾‹è¡Œï¼Œå…¶ä»–ç±»å‹ä»…æä¾›åŸºæœ¬ä¿¡æ¯ã€‚
    """
    root = Path(workspace_dir)
    if not root.exists():
        return ""

    items = []
    targets: List[Path] = []

    if selected_rel_paths:
        for rel in selected_rel_paths[:max_files]:
            try:
                p = (root / rel).resolve()
                if p.exists() and p.is_file() and root in p.parents:
                    targets.append(p)
            except Exception:
                continue
    else:
        targets = sorted([p for p in root.iterdir() if p.is_file()])[:max_files]

    for p in targets:
        try:
            rel_name = str(p.relative_to(root))
        except Exception:
            rel_name = p.name
        size_kb = (p.stat().st_size) / 1024.0
        entry = {"name": rel_name, "size": f"{size_kb:.1f}KB", "type": p.suffix.lower()}

        if p.suffix.lower() == ".csv":
            try:
                import csv
                with open(p, "r", newline="", encoding="utf-8") as f:
                    reader = csv.reader(f)
                    header = next(reader, [])
                    rows = []
                    for i, r in enumerate(reader):
                        if i >= max_rows:
                            break
                        rows.append(r)
                entry["columns"] = header
                entry["sample_rows"] = rows
            except Exception:
                entry["columns"] = []
                entry["sample_rows"] = []
        elif p.suffix.lower() == ".xlsx":
            try:
                import pandas as pd
                df = pd.read_excel(p, nrows=max_rows)
                entry["columns"] = list(df.columns)
                entry["sample_rows"] = df.head(max_rows).values.tolist()
            except Exception:
                entry["columns"] = []
                entry["sample_rows"] = []

        items.append(entry)

    try:
        return json.dumps({"files": items}, ensure_ascii=False, indent=2)
    except Exception:
        return str(items)


def _find_first_dataset(workspace_dir: str) -> Optional[Path]:
    """Find the first CSV or XLSX file in the workspace (breadth-first)."""
    root = Path(workspace_dir)
    if not root.exists():
        return None
    candidates: list[Path] = []
    for p in root.rglob("*"):
        if p.is_file() and p.suffix.lower() in {".csv", ".xlsx"}:
            candidates.append(p)
    if not candidates:
        return None
    # Prefer files not under hidden dirs
    candidates.sort(key=lambda x: ("/." in str(x), len(str(x))))
    return candidates[0]


def _simple_data_analysis(session_id: str) -> str:
    """Generate a deterministic analysis on the first dataset and return tagged content.

    This provides meaningful output when using the local Mock API, without relying on LLM.
    """
    workspace_dir = get_session_workspace(session_id)
    data_path = _find_first_dataset(workspace_dir)
    if not data_path:
        return "<Analyze>\næœªåœ¨ workspace ä¸­æ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼ˆCSV/XLSXï¼‰ã€‚è¯·å…ˆä¸Šä¼ æ•°æ®å†é‡è¯•ã€‚\n</Analyze>\n<Answer>\nNo dataset found in workspace.\n</Answer>"

    rel_path = str(Path(data_path).relative_to(Path(workspace_dir)))
    ext = data_path.suffix.lower()

    # Prefer standard library for broad compatibility
    if ext == ".csv":
        # Build executable code using csv module
        code_lines = [
            "import csv",
            f"path = r\"{rel_path}\"",
            "with open(path, 'r', newline='', encoding='utf-8') as f:",
            "    reader = csv.reader(f)",
            "    rows = list(reader)",
            "header = rows[0] if rows else []",
            "data = rows[1:] if len(rows) > 1 else []",
            "print('Rows:', len(data))",
            "print('Cols:', len(header))",
            "print('Header:', header)",
            "print('Head(5):')",
            "for r in data[:5]:",
            "    print(r)",
        ]
        code_str = "\n".join(code_lines)

        # In-process summary for Analyze
        try:
            import csv
            with open(data_path, "r", newline="", encoding="utf-8") as f:
                reader = csv.reader(f)
                rows = list(reader)
            header = rows[0] if rows else []
            data = rows[1:] if len(rows) > 1 else []
            analyze_text = (
                f"æ•°æ®æ–‡ä»¶: {rel_path}\nè¡Œæ•°: {len(data)}\nåˆ—æ•°: {len(header)}\nåˆ—å: {header}"
            )
        except Exception as e:
            analyze_text = f"CSV è§£æé”™è¯¯: {e}"
    else:
        # XLSX or others: provide guidance and minimal code path using pandas if available
        code_lines = [
            "# Optional: requires pandas and openpyxl",
            "import pandas as pd",
            f"path = r\"{rel_path}\"",
            "df = pd.read_excel(path)",
            "print('Shape:', df.shape)",
            "print('Columns:', list(df.columns))",
            "print(df.head(5).to_string())",
        ]
        code_str = "\n".join(code_lines)
        analyze_text = (
            f"æ£€æµ‹åˆ°é CSV æ–‡ä»¶: {rel_path}ã€‚å¦‚éœ€æ›´è¯¦ç»†åˆ†æï¼Œè¯·ç¡®ä¿å·²å®‰è£… pandas/openpyxlã€‚"
        )

    # Execute code in sandboxed workspace
    exe_output = execute_code_safe(code_str, workspace_dir)

    # Write a simple markdown report under generated/
    try:
        generated_dir = Path(workspace_dir) / "generated"
        generated_dir.mkdir(parents=True, exist_ok=True)
        report_path = uniquify_path(generated_dir / "data_overview.md")
        report_path.write_text(
            f"# Data Overview\n\nFile: {rel_path}\n\n## Summary\n\n{analyze_text}\n\n## Sample Head\n\n{exe_output}",
            encoding="utf-8",
        )
        report_rel = str(report_path.relative_to(Path(workspace_dir)))
        file_tag = f"<File>\n{report_rel}\n</File>\n"
    except Exception:
        file_tag = ""

    # è¡¥å…… Answer æ®µï¼Œç¡®ä¿å‰ç«¯æ­¥éª¤èƒ½å¤Ÿæ­£ç¡®æ ‡è®°å®Œæˆ
    final = (
        f"<Analyze>\n{analyze_text}\n</Analyze>\n"
        f"<Code>\n```python\n{code_str}\n```\n</Code>\n"
        f"<Execute>\n{exe_output}\n</Execute>\n"
        f"{file_tag}"
        f"<Answer>\nåˆ†æå·²å®Œæˆã€‚ä½ å¯ä»¥ç»§ç»­æå‡ºé—®é¢˜æˆ–å¯¼å‡ºæŠ¥å‘Šã€‚\n</Answer>\n"
    )
    return final


def get_file_icon(extension):
    """è·å–æ–‡ä»¶å›¾æ ‡"""
    ext = extension.lower()
    icons = {
        (".jpg", ".jpeg", ".png", ".gif", ".bmp"): "ğŸ–¼ï¸",
        (".pdf",): "ğŸ“•",
        (".doc", ".docx"): "ğŸ“˜",
        (".txt",): "ğŸ“„",
        (".md",): "ğŸ“",
        (".csv", ".xlsx"): "ğŸ“Š",
        (".json", ".sqlite"): "ğŸ—„ï¸",
        (".mp4", ".avi", ".mov"): "ğŸ¥",
        (".mp3", ".wav"): "ğŸµ",
        (".zip", ".rar", ".tar"): "ğŸ—œï¸",
    }

    for extensions, icon in icons.items():
        if ext in extensions:
            return icon
    return "ğŸ“"


def uniquify_path(target: Path) -> Path:
    """è‹¥ç›®æ ‡å·²å­˜åœ¨ï¼Œç”Ÿæˆ 'name (1).ext'ã€'name (2).ext' å½¢å¼çš„æ–°è·¯å¾„ã€‚"""
    if not target.exists():
        return target
    parent = target.parent
    stem = target.stem
    suffix = target.suffix
    import re as _re

    m = _re.match(r"^(.*) \((\d+)\)$", stem)
    base = stem
    start = 1
    if m:
        base = m.group(1)
        try:
            start = int(m.group(2)) + 1
        except Exception:
            start = 1
    i = start
    while True:
        candidate = parent / f"{base} ({i}){suffix}"
        if not candidate.exists():
            return candidate
        i += 1


def execute_code(code_str):
    """æ‰§è¡ŒPythonä»£ç """
    stdout_capture = io.StringIO()
    stderr_capture = io.StringIO()
    try:
        with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(
            stderr_capture
        ):
            exec(code_str, {})
        output = stdout_capture.getvalue()
        if stderr_capture.getvalue():
            output += stderr_capture.getvalue()
        return output
    except Exception as exec_error:
        return f"[Error]: {str(exec_error)}"


# API Routes
@app.get("/workspace/files")
async def get_workspace_files(session_id: str = Query("default")):
    """è·å–å·¥ä½œåŒºæ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¯æŒ session éš”ç¦»ï¼‰"""
    workspace_dir = get_session_workspace(session_id)
    generated_dir = Path(workspace_dir) / "generated"
    # è·å– generated ç›®å½•ä¸‹çš„æ–‡ä»¶åé›†åˆ
    generated_files = (
        set(f.name for f in generated_dir.iterdir() if f.is_file())
        if generated_dir.exists()
        else set()
    )

    files = []
    for file_path in Path(workspace_dir).iterdir():
        if file_path.is_file():
            if file_path.name in generated_files:
                continue
            stat = file_path.stat()
            rel_path = f"{session_id}/{file_path.name}"
            files.append(
                {
                    "name": file_path.name,
                    "size": stat.st_size,
                    "extension": file_path.suffix.lower(),
                    "icon": get_file_icon(file_path.suffix),
                    "download_url": build_download_url(rel_path),
                    "preview_url": (
                        build_download_url(rel_path)
                        if file_path.suffix.lower()
                        in [
                            ".jpg",
                            ".jpeg",
                            ".png",
                            ".gif",
                            ".bmp",
                            ".pdf",
                            ".txt",
                            ".doc",
                            ".docx",
                            ".csv",
                            ".xlsx",
                        ]
                        else None
                    ),
                }
            )
    return {"files": files}


# ---------- Workspace Tree & Single File Delete ----------
def _rel_path(path: Path, root: Path) -> str:
    try:
        rel = path.relative_to(root)
        return rel.as_posix()
    except Exception:
        return path.name


def build_tree(path: Path, root: Path | None = None) -> dict:
    if root is None:
        root = path
    node: dict = {
        "name": path.name or "workspace",
        "path": _rel_path(path, root),
        "is_dir": path.is_dir(),
    }
    if path.is_dir():
        children = []

        # è‡ªå®šä¹‰æ’åºï¼šgenerated æ–‡ä»¶å¤¹æ”¾åœ¨æœ€åï¼Œå…¶ä»–æŒ‰ç›®å½•ä¼˜å…ˆã€åç§°æ’åº
        def sort_key(p):
            is_generated = p.name == "generated"
            is_dir = p.is_dir()
            return (is_generated, not is_dir, p.name.lower())

        for child in sorted(path.iterdir(), key=sort_key):
            if child.name.startswith("."):
                continue
            children.append(build_tree(child, root))
        node["children"] = children
    else:
        node["size"] = path.stat().st_size
        node["extension"] = path.suffix.lower()
        node["icon"] = get_file_icon(path.suffix)
        rel = _rel_path(path, root)
        node["download_url"] = build_download_url(rel)
    return node


@app.get("/workspace/tree")
async def workspace_tree(session_id: str = Query("default")):
    workspace_dir = get_session_workspace(session_id)
    root = Path(workspace_dir)
    tree_data = build_tree(root, root)

    # åœ¨ä¸‹è½½é“¾æ¥å‰åŠ ä¸Š session_id å‰ç¼€
    def prefix_urls(node, sid):
        if "download_url" in node and node["download_url"]:
            # é‡æ–°æ„å»ºåŒ…å« session_id çš„è·¯å¾„
            rel = node.get("path", "")
            node["download_url"] = build_download_url(f"{sid}/{rel}")
        if "children" in node:
            for child in node["children"]:
                prefix_urls(child, sid)

    prefix_urls(tree_data, session_id)
    return tree_data


@app.delete("/workspace/file")
async def delete_workspace_file(
    path: str = Query(..., description="relative path under workspace"),
    session_id: str = Query("default"),
):
    workspace_dir = get_session_workspace(session_id)
    abs_workspace = Path(workspace_dir).resolve()
    target = (abs_workspace / path).resolve()
    if abs_workspace not in target.parents and target != abs_workspace:
        raise HTTPException(status_code=400, detail="Invalid path")
    if not target.exists():
        raise HTTPException(status_code=404, detail="Not found")
    if target.is_dir():
        raise HTTPException(status_code=400, detail="Folder deletion not allowed")
    try:
        target.unlink()
        return {"message": "deleted"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/workspace/move")
async def move_path(
    src: str = Query(..., description="relative source path under workspace"),
    dst_dir: str = Query("", description="relative target directory under workspace"),
    session_id: str = Query("default"),
):
    """åœ¨åŒä¸€ workspace å†…ç§»åŠ¨ï¼ˆæˆ–é‡å‘½åï¼‰æ–‡ä»¶/ç›®å½•ã€‚
    - src: æºç›¸å¯¹è·¯å¾„ï¼ˆå¿…å¡«ï¼‰
    - dst_dir: ç›®æ ‡ç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ï¼Œç©ºè¡¨ç¤ºç§»åŠ¨åˆ°æ ¹ç›®å½•ï¼‰
    """
    workspace_dir = get_session_workspace(session_id)
    abs_workspace = Path(workspace_dir).resolve()

    abs_src = (abs_workspace / src).resolve()
    if abs_workspace not in abs_src.parents and abs_src != abs_workspace:
        raise HTTPException(status_code=400, detail="Invalid src path")
    if not abs_src.exists():
        raise HTTPException(status_code=404, detail="Source not found")

    abs_dst_dir = (abs_workspace / (dst_dir or "")).resolve()
    if abs_workspace not in abs_dst_dir.parents and abs_dst_dir != abs_workspace:
        raise HTTPException(status_code=400, detail="Invalid dst_dir path")
    abs_dst_dir.mkdir(parents=True, exist_ok=True)

    target = abs_dst_dir / abs_src.name
    target = uniquify_path(target)
    try:
        shutil.move(str(abs_src), str(target))
        rel_new = str(target.relative_to(abs_workspace))
        return {"message": "moved", "new_path": rel_new}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Move failed: {e}")


@app.delete("/workspace/dir")
async def delete_workspace_dir(
    path: str = Query(..., description="relative directory under workspace"),
    recursive: bool = Query(True, description="delete directory recursively"),
    session_id: str = Query("default"),
):
    """åˆ é™¤ workspace ä¸‹çš„ç›®å½•ã€‚é»˜è®¤é€’å½’åˆ é™¤ï¼Œç¦æ­¢åˆ é™¤æ ¹ç›®å½•ã€‚"""
    workspace_dir = get_session_workspace(session_id)
    abs_workspace = Path(workspace_dir).resolve()
    target = (abs_workspace / path).resolve()
    if abs_workspace not in target.parents and target != abs_workspace:
        raise HTTPException(status_code=400, detail="Invalid path")
    if target == abs_workspace:
        raise HTTPException(status_code=400, detail="Cannot delete workspace root")
    if not target.exists():
        raise HTTPException(status_code=404, detail="Not found")
    if not target.is_dir():
        raise HTTPException(status_code=400, detail="Not a directory")
    try:
        if recursive:
            shutil.rmtree(target)
        else:
            target.rmdir()
        return {"message": "deleted"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/proxy")
async def proxy(url: str):
    """Simple CORS proxy for previewing external files.
    WARNING: For production, add domain allowlist and authentication.
    """
    try:
        async with httpx.AsyncClient(follow_redirects=True, timeout=15) as client:
            r = await client.get(url)
        return Response(
            content=r.content,
            media_type=r.headers.get("content-type", "application/octet-stream"),
            headers={"Access-Control-Allow-Origin": "*"},
            status_code=r.status_code,
        )
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"Proxy fetch failed: {e}")


@app.post("/workspace/upload")
async def upload_files(
    files: List[UploadFile] = File(...), session_id: str = Query("default")
):
    """ä¸Šä¼ æ–‡ä»¶åˆ°å·¥ä½œåŒºï¼ˆæ”¯æŒ session éš”ç¦»ï¼‰"""
    workspace_dir = get_session_workspace(session_id)
    uploaded_files = []

    for file in files:
        # å”¯ä¸€åŒ–æ–‡ä»¶åï¼Œé¿å…è¦†ç›–
        dst = uniquify_path(Path(workspace_dir) / file.filename)
        with open(dst, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        uploaded_files.append(
            {
                "name": dst.name,
                "size": len(content),
                "path": str(dst.relative_to(Path(workspace_dir))),
            }
        )

    return {
        "message": f"Successfully uploaded {len(uploaded_files)} files",
        "files": uploaded_files,
    }


@app.delete("/workspace/clear")
async def clear_workspace(session_id: str = Query("default")):
    """æ¸…ç©ºå·¥ä½œåŒºï¼ˆæ”¯æŒ session éš”ç¦»ï¼‰"""
    workspace_dir = get_session_workspace(session_id)
    if os.path.exists(workspace_dir):
        shutil.rmtree(workspace_dir)
    os.makedirs(workspace_dir, exist_ok=True)
    return {"message": "Workspace cleared successfully"}


@app.post("/workspace/upload-to")
async def upload_to_dir(
    dir: str = Query("", description="relative directory under workspace"),
    files: List[UploadFile] = File(...),
    session_id: str = Query("default"),
):
    """ä¸Šä¼ æ–‡ä»¶åˆ° workspace ä¸‹çš„æŒ‡å®šå­ç›®å½•ï¼ˆä»…é™å·¥ä½œåŒºå†…ï¼‰ã€‚"""
    workspace_dir = get_session_workspace(session_id)
    abs_workspace = Path(workspace_dir).resolve()
    target_dir = (abs_workspace / dir).resolve()
    if abs_workspace not in target_dir.parents and target_dir != abs_workspace:
        raise HTTPException(status_code=400, detail="Invalid dir path")
    target_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        dst = uniquify_path(target_dir / f.filename)
        try:
            with open(dst, "wb") as buffer:
                content = await f.read()
                buffer.write(content)
            saved.append(
                {
                    "name": dst.name,
                    "size": len(content),
                    "path": str(dst.relative_to(abs_workspace)),
                }
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Save failed: {e}")
    return {"message": f"uploaded {len(saved)}", "files": saved}


@app.post("/execute")
async def execute_code_api(request: dict):
    """æ‰§è¡Œ Python ä»£ç """
    print("ğŸ”¥ Execute API called:", request)  # Debug log

    try:
        code = request.get("code", "")
        session_id = request.get("session_id", "default")
        workspace_dir = get_session_workspace(session_id)

        if not code:
            raise HTTPException(status_code=400, detail="No code provided")

        print(f"Executing code: {code[:100]}...")  # Debug log (first 100 chars)

        # ä½¿ç”¨å­è¿›ç¨‹å®‰å…¨æ‰§è¡Œï¼Œé¿å… GUI/çº¿ç¨‹é—®é¢˜ï¼ˆåœ¨æŒ‡å®š session workspace ä¸­ï¼‰
        result = execute_code_safe(code, workspace_dir)
        print(f"âœ… Execution result: {result[:200]}...")  # Debug log

        return {
            "success": True,
            "result": result,
            "message": "Code executed successfully",
        }

    except Exception as e:
        print(f"âŒ Execution error: {traceback.format_exc()}")  # Debug log
        return {
            "success": False,
            "result": f"Error: {str(e)}",
            "message": "Code execution failed",
        }


def fix_code_block(content):
    def fix_text(text):
        stack = []
        lines = text.splitlines(keepends=True)
        result = []
        for line in lines:
            stripped = line.strip()
            if stripped.startswith("```python"):
                if stack and stack[-1] == "```python":
                    result.append("```\n")
                    stack.pop()
                stack.append("```python")
                result.append(line)
            elif stripped == "```":
                if stack and stack[-1] == "```python":
                    stack.pop()
                result.append(line)
            else:
                result.append(line)
        while stack:
            result.append("```\n")
            stack.pop()
        return "".join(result)

    if isinstance(content, str):
        return fix_text(content)
    elif isinstance(content, tuple):
        text_part = content[0] if content[0] else ""
        return (fix_text(text_part), content[1])
    return content


def fix_tags_and_codeblock(s: str) -> str:
    """
    ä¿®å¤æœªé—­åˆçš„tagsï¼Œå¹¶ç¡®ä¿</Code>åä»£ç å—é—­åˆã€‚
    """
    pattern = re.compile(
        r"<(Analyze|Understand|Code|Execute|Answer)>(.*?)(?:</\1>|(?=$))", re.DOTALL
    )

    # æ‰¾æ‰€æœ‰åŒ¹é…
    matches = list(pattern.finditer(s))
    if not matches:
        return s  # æ²¡æœ‰æ ‡ç­¾ï¼Œç›´æ¥è¿”å›

    # æ£€æŸ¥æœ€åä¸€ä¸ªåŒ¹é…æ˜¯å¦é—­åˆ
    last_match = matches[-1]
    tag_name = last_match.group(1)
    matched_text = last_match.group(0)

    if not matched_text.endswith(f"</{tag_name}>"):
        # æ²¡æœ‰é—­åˆæ—¶è°¨æ…è¡¥é½ï¼Œä»…å½“å­˜åœ¨èµ·å§‹æ ‡ç­¾æ‰è¡¥
        if tag_name == "Code":
            # è‹¥æ–‡æœ¬ä¸­å­˜åœ¨ <Code> ä½†ç¼ºå°‘ </Code>ï¼Œä¸”ä»£ç å—æœªé—­åˆï¼Œåˆ™è¡¥é½
            if "<Code>" in s and "</Code>" not in s:
                s = fix_code_block(s) + f"\n</{tag_name}>"
        else:
            s += f"\n</{tag_name}>"

    return s


def bot_stream(messages, workspace, session_id="default", language: str = "zh"):
    original_cwd = os.getcwd()
    WORKSPACE_DIR = get_session_workspace(session_id)
    os.makedirs(WORKSPACE_DIR, exist_ok=True)
    # åˆ›å»º generated å­æ–‡ä»¶å¤¹ç”¨äºå­˜æ”¾ä»£ç ç”Ÿæˆçš„æ–‡ä»¶
    GENERATED_DIR = os.path.join(WORKSPACE_DIR, "generated")
    os.makedirs(GENERATED_DIR, exist_ok=True)
    # print(messages)
    if messages and messages[0]["role"] == "assistant":
        messages = messages[1:]
    if messages and messages[-1]["role"] == "user":
        user_message = messages[-1]["content"]
        # workspace å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„åˆ—è¡¨ï¼›æˆ‘ä»¬å°†å…¶ç”¨äºæ„å»ºç²¾ç»†çš„æ•°æ®ä¸Šä¸‹æ–‡
        try:
            selected_paths = [str(p) for p in workspace] if isinstance(workspace, list) else []
        except Exception:
            selected_paths = []

        file_info_rich = collect_rich_data_context_for(WORKSPACE_DIR, selected_paths)
        # è‹¥æœªé€‰æ‹©æ–‡ä»¶æˆ–è§£æå¤±è´¥ï¼Œé€€å›åˆ°åŸºç¡€ä¿¡æ¯
        if not file_info_rich:
            file_info_rich = collect_rich_data_context(WORKSPACE_DIR)

        if file_info_rich:
            messages[-1]["content"] = (
                f"# UserQuestion\n{user_message}\n\n"
                f"# DataContext\n{file_info_rich}"
            )
        else:
            messages[-1]["content"] = f"# UserQuestion\n{user_message}"

    # åœ¨æœ€å‰æ³¨å…¥ç³»ç»Ÿæç¤ºï¼ŒæŒ‡å¯¼å¤§æ¨¡å‹ç”Ÿæˆå¯æ‰§è¡Œã€å¯è§†åŒ–ä¸”é²æ£’çš„ Python ä»£ç 
    # æ ¹æ®è°ƒç”¨æ–¹ä¼ å…¥çš„ language åˆ‡æ¢ç³»ç»Ÿæç¤ºè¯
    lang = (language or "zh").lower()

    if lang.startswith("zh"):
        system_prompt = (
            "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ•°æ®åˆ†æåŠ©æ‰‹ã€‚è¯·åŒæ—¶è€ƒè™‘ç”¨æˆ·è¯¢é—® (# UserQuestion) ä¸æ•°æ®ä¸Šä¸‹æ–‡ (# DataContext)ã€‚"
            "åŠ¡å¿…ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š<Analyze>â€¦</Analyze><Code>```python\nâ€¦\n```</Code>ã€‚"
            "è¦æ±‚ï¼š1) ä»£ç ä»…ä½¿ç”¨æ ‡å‡†åº“æˆ– pandas/numpy/matplotlibï¼ˆå¯é€‰ï¼‰ï¼Œ"
            "2) ç¦æ­¢ä½¿ç”¨é™¤ä¸Šè¿°ä¹‹å¤–çš„ç¬¬ä¸‰æ–¹åº“ï¼ˆå¦‚ scikit-learnã€seabornã€xgboostã€tensorflowã€torch ç­‰ï¼‰ï¼Œ"
            "3) è‡ªåŠ¨å¤„ç†ç¼ºå¤±å€¼ä¸å¼‚å¸¸ï¼›4) ç»™å‡ºç»Ÿè®¡æ‘˜è¦ä¸å¯è§†åŒ–å»ºè®®ï¼›"
            "5) ä¸è¦ä½¿ç”¨ notebook é­”æ³•å‘½ä»¤ï¼ˆä¾‹å¦‚ %pwdï¼‰ï¼›6) è¯»å–æ–‡ä»¶è·¯å¾„å¿…é¡»æ¥è‡ª # DataContext çš„ files åˆ—è¡¨ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰ã€‚"
        )
    else:
        system_prompt = (
            "You are a senior data analysis assistant. Consider both # UserQuestion and # DataContext."
            " Strictly output in the format: <Analyze>â€¦</Analyze><Code>```python\nâ€¦\n```</Code>."
            " Requirements: 1) Use only standard library or pandas/numpy/matplotlib (optional),"
            " 2) Do NOT use third-party libraries beyond these (e.g., scikit-learn, seaborn, xgboost, tensorflow, torch),"
            " 3) Handle missing/outlier values; 4) Provide statistical summaries and visualization suggestions;"
            " 5) Avoid notebook magic commands (e.g., %pwd); 6) File paths must come from files listed in # DataContext (relative paths)."
        )
    messages = ([{"role": "system", "content": system_prompt}] + messages)
    # print("111",messages)
    initial_workspace = set(workspace)
    assistant_reply = ""
    finished = False
    exe_output = None
    while not finished:
        # Decide streaming capability by API base (mock server doesn't stream)
        use_stream = "localhost:8000" not in API_BASE
        if use_stream:
            # Streaming path
            response = client.chat.completions.create(
                model=MODEL_PATH,
                messages=messages,
                temperature=0.4,
                stream=True,
                extra_body={
                    "add_generation_prompt": False,
                    "stop_token_ids": [151676, 151645],
                    "max_new_tokens": 32768,
                },
            )
            cur_res = ""
            last_finish_reason = None
            for rchunk in response:
                if rchunk.choices:
                    if getattr(rchunk.choices[0], "delta", None):
                        delta = rchunk.choices[0].delta.content
                        if delta is not None:
                            cur_res += delta
                            assistant_reply += delta
                            yield assistant_reply
                    if rchunk.choices[0].finish_reason:
                        last_finish_reason = rchunk.choices[0].finish_reason
            # ä¸å†ç›²ç›®è¡¥é½ </Code>ï¼Œä»…åœ¨å­˜åœ¨ <Code> ä¸”ç¼ºå°‘é—­åˆæ—¶è¡¥é½
            if (
                last_finish_reason == "stop"
                and ("<Code>" in cur_res)
                and ("</Code>" not in cur_res)
            ):
                cur_res += "</Code>"
                assistant_reply += "</Code>"
                yield assistant_reply
            finished = True
        else:
            # Non-stream path (mock server)
            response = client.chat.completions.create(
                model=MODEL_PATH,
                messages=messages,
                temperature=0.4,
                stream=False,
                extra_body={
                    "add_generation_prompt": False,
                    "stop_token_ids": [151676, 151645],
                    "max_new_tokens": 32768,
                },
            )
            try:
                msg = response.choices[0].message
                content = msg.get("content", "") if isinstance(msg, dict) else getattr(msg, "content", "")
            except Exception:
                content = ""
            cur_res = content
            assistant_reply += content
            yield assistant_reply
            finished = True
            if "</Answer>" in assistant_reply:
                finished = True
                break
        # æ‰§è¡Œä»£ç æ®µï¼ˆå³ä½¿æµå¼å·²ç»“æŸä¹Ÿéœ€è¦æ‰§è¡Œï¼‰
        if "</Code>" in cur_res:
            messages.append({"role": "assistant", "content": cur_res})
            code_match = re.search(r"<Code>(.*?)</Code>", cur_res, re.DOTALL)
            if code_match:
                code_content = code_match.group(1).strip()
                md_match = re.search(r"```(?:python)?(.*?)```", code_content, re.DOTALL)
                code_str = md_match.group(1).strip() if md_match else code_content
                # æ‰§è¡Œå‰å¿«ç…§ï¼ˆè·¯å¾„ -> (size, mtime)ï¼‰
                try:
                    before_state = {
                        p.resolve(): (p.stat().st_size, p.stat().st_mtime_ns)
                        for p in Path(WORKSPACE_DIR).rglob("*")
                        if p.is_file()
                    }
                except Exception:
                    before_state = {}
                # åœ¨å­è¿›ç¨‹ä¸­ä»¥å›ºå®šå·¥ä½œåŒºæ‰§è¡Œï¼ˆæµå¼ï¼‰
                stream_started = False
                exe_collected = []
                for chunk in execute_code_stream(code_str, WORKSPACE_DIR):
                    if not stream_started:
                        # é¦–æ¬¡å—ï¼šå¼€å§‹ Execute æ®µï¼ˆä½¿ç”¨ä¸‰åå¼•å·ä¾›å‰ç«¯é«˜äº®ï¼‰
                        assistant_reply += "\n<Execute>\n```\n"
                        stream_started = True
                    # è¿½åŠ è¾“å‡º
                    assistant_reply += chunk
                    exe_collected.append(chunk)
                    yield assistant_reply
                # ç»“æŸ Execute æ®µ
                if stream_started:
                    assistant_reply += "\n```\n</Execute>\n"
                exe_output = "".join(exe_collected)

                # æ‰§è¡Œåå¿«ç…§
                try:
                    after_state = {
                        p.resolve(): (p.stat().st_size, p.stat().st_mtime_ns)
                        for p in Path(WORKSPACE_DIR).rglob("*")
                        if p.is_file()
                    }
                except Exception:
                    after_state = {}
                # è®¡ç®—æ–°å¢ä¸ä¿®æ”¹
                added_paths = [p for p in after_state.keys() if p not in before_state]
                modified_paths = [
                    p
                    for p in after_state.keys()
                    if p in before_state and after_state[p] != before_state[p]
                ]

                # å°†æ–°å¢å’Œä¿®æ”¹çš„æ–‡ä»¶ç§»åŠ¨åˆ° generated æ–‡ä»¶å¤¹
                artifact_paths = []
                for p in added_paths:
                    try:
                        # å¦‚æœæ–‡ä»¶ä¸åœ¨ generated æ–‡ä»¶å¤¹ä¸­ï¼Œç§»åŠ¨å®ƒ
                        if not str(p).startswith(GENERATED_DIR):
                            dest_path = Path(GENERATED_DIR) / p.name
                            dest_path = uniquify_path(dest_path)
                            shutil.copy2(str(p), str(dest_path))
                            artifact_paths.append(dest_path.resolve())
                        else:
                            artifact_paths.append(p)
                    except Exception as e:
                        print(f"Error moving file {p}: {e}")
                        artifact_paths.append(p)

                # ä¸ºä¿®æ”¹çš„æ–‡ä»¶ç”Ÿæˆå‰¯æœ¬å¹¶ç§»åŠ¨åˆ° generated æ–‡ä»¶å¤¹
                for p in modified_paths:
                    try:
                        dest_name = f"{Path(p).stem}_modified{Path(p).suffix}"
                        dest_path = Path(GENERATED_DIR) / dest_name
                        dest_path = uniquify_path(dest_path)
                        shutil.copy2(p, dest_path)
                        artifact_paths.append(dest_path.resolve())
                    except Exception as e:
                        print(f"Error copying modified file {p}: {e}")

                # æ—§ï¼šExecute å†…éƒ¨æ”¾æ§åˆ¶å°è¾“å‡ºï¼›æ–°ï¼šè¿½åŠ  <File> æ®µè½ç»™å‰ç«¯æ¸²æŸ“å¡ç‰‡
                exe_str = ""  # å·²é€šè¿‡æµå¼è¾“å‡ºåˆ° assistant_replyï¼Œè¿™é‡Œä¸é‡å¤è¾“å‡º
                file_block = ""
                if artifact_paths:
                    lines = ["<File>"]
                    for p in artifact_paths:
                        try:
                            rel = (
                                Path(p)
                                .relative_to(Path(WORKSPACE_DIR).resolve())
                                .as_posix()
                            )
                        except Exception:
                            rel = Path(p).name
                        # åœ¨ç›¸å¯¹è·¯å¾„å‰åŠ ä¸Š session_id å‰ç¼€
                        url = build_download_url(f"{session_id}/{rel}")
                        name = Path(p).name
                        lines.append(f"- [{name}]({url})")
                        if Path(p).suffix.lower() in [
                            ".png",
                            ".jpg",
                            ".jpeg",
                            ".gif",
                            ".webp",
                            ".svg",
                        ]:
                            lines.append(f"![{name}]({url})")
                    lines.append("</File>")
                    file_block = "\n" + "\n".join(lines) + "\n"
                assistant_reply += exe_str + file_block
                # è‹¥æ¨¡å‹æœªæä¾› <Answer>ï¼Œåœ¨æ‰§è¡Œå®Œæˆåè¡¥å……ä¸€ä¸ªç®€çŸ­çš„ç»“è®ºï¼Œä¾¿äºå‰ç«¯æµç¨‹ç»“æŸ
                if "</Answer>" not in assistant_reply:
                    assistant_reply += (
                        "\n<Answer>\næ‰§è¡Œå·²å®Œæˆï¼Œç»“æœä¸ç”Ÿæˆæ–‡ä»¶å·²å±•ç¤ºã€‚è‹¥éœ€ç»§ç»­åˆ†ææˆ–å¯¼å‡ºæŠ¥å‘Šï¼Œè¯·å‘ŠçŸ¥ã€‚\n</Answer>\n"
                    )
                yield assistant_reply
                messages.append({"role": "execute", "content": f"{exe_output}"})
                # åˆ·æ–°å·¥ä½œåŒºå¿«ç…§ï¼ˆè·¯å¾„é›†åˆï¼‰
                current_files = set(
                    [
                        os.path.join(WORKSPACE_DIR, f)
                        for f in os.listdir(WORKSPACE_DIR)
                        if os.path.isfile(os.path.join(WORKSPACE_DIR, f))
                    ]
                )
                new_files = list(current_files - initial_workspace)
                if new_files:
                    workspace.extend(new_files)
                    initial_workspace.update(new_files)
    os.chdir(original_cwd)


@app.post("/chat/completions")
async def chat(body: dict = Body(...)):
    messages = body.get("messages", [])
    workspace = body.get("workspace", [])
    session_id = body.get("session_id", "default")
    stream_flag = bool(body.get("stream", False))
    # Detect language from request body, default to Chinese
    lang = str(body.get("language") or "zh").lower()

    # When stream=True â†’ return text/plain streaming of OpenAI-style JSON objects
    if stream_flag:
        def generate():
            for reply in bot_stream(messages, workspace, session_id, language=lang):
                print(reply)
                result = {
                    "id": "chatcmpl-123",
                    "object": "chat.completion",
                    "created": 1677652288,
                    "model": "deepanalyze-8b",
                    "choices": [
                        {
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": fix_tags_and_codeblock(reply),
                            },
                            "finish_reason": "stop",
                        }
                    ],
                }
                yield json.dumps(result)

        return StreamingResponse(generate(), media_type="text/plain")

    # When stream=false â†’ return a single JSON object with full content
    # Special-case: if using local mock API, fetch directly to ensure content
    final_reply = ""
    if "localhost:8000" in API_BASE:
        try:
            # Build language-specific system prompt for mock server path
            if lang.startswith("zh"):
                system_prompt = (
                    "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ•°æ®åˆ†æåŠ©æ‰‹ã€‚è¯·åŒæ—¶è€ƒè™‘ç”¨æˆ·è¯¢é—® (# UserQuestion) ä¸æ•°æ®ä¸Šä¸‹æ–‡ (# DataContext)ã€‚"
                    "åŠ¡å¿…ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š<Analyze>â€¦</Analyze><Code>```python\nâ€¦\n```</Code>ã€‚"
                    "è¦æ±‚ï¼š1) ä»£ç ä»…ä½¿ç”¨æ ‡å‡†åº“æˆ– pandas/numpy/matplotlibï¼ˆå¯é€‰ï¼‰ï¼Œ"
                    "2) ç¦æ­¢ä½¿ç”¨é™¤ä¸Šè¿°ä¹‹å¤–çš„ç¬¬ä¸‰æ–¹åº“ï¼ˆå¦‚ scikit-learnã€seabornã€xgboostã€tensorflowã€torch ç­‰ï¼‰ï¼Œ"
                    "3) è‡ªåŠ¨å¤„ç†ç¼ºå¤±å€¼ä¸å¼‚å¸¸ï¼›4) ç»™å‡ºç»Ÿè®¡æ‘˜è¦ä¸å¯è§†åŒ–å»ºè®®ï¼›"
                    "5) ä¸è¦ä½¿ç”¨ notebook é­”æ³•å‘½ä»¤ï¼ˆä¾‹å¦‚ %pwdï¼‰ï¼›6) è¯»å–æ–‡ä»¶è·¯å¾„å¿…é¡»æ¥è‡ª # DataContext çš„ files åˆ—è¡¨ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰ã€‚"
                )
            else:
                system_prompt = (
                    "You are a senior data analysis assistant. Consider both # UserQuestion and # DataContext."
                    " Strictly output in the format: <Analyze>â€¦</Analyze><Code>```python\nâ€¦\n```</Code>."
                    " Requirements: 1) Use only standard library or pandas/numpy/matplotlib (optional),"
                    " 2) Do NOT use third-party libraries beyond these (e.g., scikit-learn, seaborn, xgboost, tensorflow, torch),"
                    " 3) Handle missing/outlier values; 4) Provide statistical summaries and visualization suggestions;"
                    " 5) Avoid notebook magic commands (e.g., %pwd); 6) File paths must come from files listed in # DataContext (relative paths)."
                )
            messages_with_system = ([{"role": "system", "content": system_prompt}] + messages)
            resp = client.chat.completions.create(
                model=MODEL_PATH,
                messages=messages_with_system,
                temperature=0.4,
                stream=False,
                extra_body={"add_generation_prompt": False},
            )
            msg = resp.choices[0].message
            final_reply = msg.get("content", "") if isinstance(msg, dict) else getattr(msg, "content", "")
        except Exception:
            final_reply = ""
        # Fallback: if still empty, use internal generator to synthesize reply
        if not final_reply or not str(final_reply).strip():
            # Prefer deterministic data analysis over LLM when using mock server
            try:
                final_reply = _simple_data_analysis(session_id)
            except Exception:
                # as ultimate fallback, attempt internal generator
                try:
                    for reply in bot_stream(messages, workspace, session_id, language=lang):
                        final_reply = reply
                except Exception:
                    final_reply = ""
        else:
            # If we have dataset and mock reply lacks Execute/File, upgrade to real analysis
            try:
                workspace_dir = get_session_workspace(session_id)
                if _find_first_dataset(workspace_dir) and (
                    ("</Execute>" not in final_reply) or ("</File>" not in final_reply)
                ):
                    final_reply = _simple_data_analysis(session_id)
            except Exception:
                pass
    else:
        for reply in bot_stream(messages, workspace, session_id, language=lang):
            final_reply = reply

    result = {
        "id": "chatcmpl-123",
        "object": "chat.completion",
        "created": 1677652288,
        "model": "deepanalyze-8b",
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": fix_tags_and_codeblock(final_reply),
                },
                "finish_reason": "stop",
            }
        ],
    }
    return JSONResponse(result)


# -------- Export Report (PDF + MD) --------
from datetime import datetime


def _extract_sections_from_messages(messages: list[dict]) -> str:
    """ä»å†å²æ¶ˆæ¯ä¸­æŠ½å– <Answer>..</Answer> ä½œä¸ºæŠ¥å‘Šä¸»ä½“ï¼Œå…¶ä½™éƒ¨åˆ†æŒ‰åŸå§‹é¡ºåºä½œä¸º Appendix æ‹¼æˆ Markdownã€‚"""
    if not isinstance(messages, list):
        return ""
    import re as _re

    parts: list[str] = []
    appendix: list[str] = []

    tag_pattern = r"<(Analyze|Understand|Code|Execute|File|Answer)>([\s\S]*?)</\1>"

    for idx, m in enumerate(messages, start=1):
        role = (m or {}).get("role")
        if role != "assistant":
            continue
        content = str((m or {}).get("content") or "")

        step = 1
        # æŒ‰ç…§åœ¨æ–‡æœ¬ä¸­çš„å‡ºç°é¡ºåºä¾æ¬¡æå–
        for match in _re.finditer(tag_pattern, content, _re.DOTALL):
            tag, seg = match.groups()
            seg = seg.strip()
            if tag == "Answer":
                parts.append(f"{seg}\n")

            appendix.append(f"\n### Step {step}: {tag}\n\n{seg}\n")
            step += 1

    final_text = "".join(parts).strip()
    if appendix:
        final_text += (
            "\n\n\\newpage\n\n# Appendix: Detailed Process\n"
            + "".join(appendix).strip()
        )

    # print(final_text)
    return final_text


def _save_md(md_text: str, base_name: str, workspace_dir: str) -> Path:
    Path(workspace_dir).mkdir(parents=True, exist_ok=True)
    md_path = uniquify_path(Path(workspace_dir) / f"{base_name}.md")
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(md_text)
    return md_path


import pypandoc


def _save_pdf(md_text: str, base_name: str, workspace_dir: str) -> Path:
    Path(workspace_dir).mkdir(parents=True, exist_ok=True)
    pdf_path = uniquify_path(Path(workspace_dir) / f"{base_name}.pdf")

    output = pypandoc.convert_text(
        md_text,
        "pdf",
        format="md",
        outputfile=str(pdf_path),
        extra_args=[
            "--standalone",
            "--pdf-engine=xelatex",
        ],
    )
    return pdf_path


from typing import Optional


def _render_md_to_html(md_text: str, title: Optional[str] = None) -> str:
    """ç®€åŒ–ä¸ºå ä½å®ç°ï¼ˆä»…ä¾›æœªæ¥ PDF æ¸²æŸ“ä½¿ç”¨ï¼‰ã€‚å½“å‰ä»…ç”Ÿæˆ MDã€‚"""
    doc_title = (title or "Report").strip() or "Report"
    safe = (md_text or "").replace("<", "&lt;").replace(">", "&gt;")
    return f"<html><head><meta charset='utf-8'><title>{doc_title}</title></head><body><pre>{safe}</pre></body></html>"


def _save_pdf_from_md(html_text: str, base_name: str) -> Path:
    """TODO: æœåŠ¡ç«¯ PDF æ¸²æŸ“æœªå®ç°ã€‚"""
    raise NotImplementedError("TODO: implement server-side PDF rendering")


def _save_pdf_with_chromium(html_text: str, base_name: str) -> Path:
    """TODO: ä½¿ç”¨ Chromium æ¸²æŸ“ PDFï¼ˆæš‚ä¸å®ç°ï¼‰ã€‚"""
    raise NotImplementedError("TODO: chromium-based PDF rendering")


def _save_pdf_from_text(text: str, base_name: str) -> Path:
    """TODO: çº¯æ–‡æœ¬ PDF æ¸²æŸ“ï¼ˆæš‚ä¸å®ç°ï¼‰ã€‚"""
    raise NotImplementedError("TODO: text-based PDF rendering")


@app.post("/export/report")
async def export_report(body: dict = Body(...)):
    """
    æ¥æ”¶å…¨éƒ¨èŠå¤©å†å²ï¼ˆmessages: [{role, content}...]ï¼‰ï¼ŒæŠ½å– <Analyze>..</Analyze> ~ <Answer>..</Answer>
    ä»…ç”Ÿæˆ Markdown æ–‡ä»¶å¹¶ä¿å­˜åˆ° workspaceï¼›PDF æ¸²æŸ“ç•™ä½œ TODOã€‚
    """
    try:
        messages = body.get("messages", [])
        title = (body.get("title") or "").strip()
        session_id = body.get("session_id", "default")
        workspace_dir = get_session_workspace(session_id)

        if not isinstance(messages, list):
            raise HTTPException(status_code=400, detail="messages must be a list")

        md_text = _extract_sections_from_messages(messages)
        if not md_text:
            md_text = (
                "(No <Analyze>/<Understand>/<Code>/<Execute>/<Answer> sections found.)"
            )

        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_title = re.sub(r"[^\w\-_.]+", "_", title) if title else "Report"
        base_name = f"{safe_title}_{ts}" if title else f"Report_{ts}"

        # Save MD into generated/ folder under workspace
        export_dir = os.path.join(workspace_dir, "generated")
        os.makedirs(export_dir, exist_ok=True)

        print(md_text)
        md_path = _save_md(md_text, base_name, export_dir)

        # PDF æš‚ä¸ç”Ÿæˆï¼ˆTODOï¼‰ã€‚
        pdf_path = _save_pdf(md_text, base_name, export_dir)

        result = {
            "message": "exported",
            "md": md_path.name,
            "pdf": pdf_path.name if pdf_path else None,
            "download_urls": {
                "md": build_download_url(f"{session_id}/generated/{md_path.name}"),
                "pdf": (
                    build_download_url(f"{session_id}/generated/{pdf_path.name}")
                    if pdf_path
                    else None
                ),
            },
        }
        return JSONResponse(result)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨åç«¯æœåŠ¡...")
    print(f"   - APIæœåŠ¡: http://localhost:8200")
    print(f"   - æ–‡ä»¶æœåŠ¡: http://localhost:8100")
    uvicorn.run(app, host="0.0.0.0", port=8200)
